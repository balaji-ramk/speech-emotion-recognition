{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Recognition and Classification in Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def create_ravdess_map_df():\n",
    "    file_paths = []\n",
    "    emotions = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(\"Audio_Speech_Actors_01-24\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                emotion_code = int(file.split('-')[2])\n",
    "                emotion_map = {1: \"neutral\", 2: \"calm\", 3: \"happy\", 4: \"sad\", 5: \"angry\", 6: \"fearful\", 7: \"disgust\", 8: \"surprised\"}\n",
    "                emotion = emotion_map[emotion_code]\n",
    "                \n",
    "                file_paths.append(file_path)\n",
    "                emotions.append(emotion)\n",
    "    \n",
    "    df = pd.DataFrame({\"file_path\": file_paths, \"emotion\": emotions})\n",
    "    df.to_csv(\"ravdess_metadata.csv\", index=False)\n",
    "    print(f\"Dataset prepared with {len(df)} audio files.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ravdess_df = create_ravdess_map_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def add_transcripts_to_ravdess(df, output_dir=\"transcripts\", model_size=\"distil-large-v3\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    transcripts = []\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = WhisperModel(model_size, device=device, compute_type=\"float16\" if torch.cuda.is_available() else \"float32\")\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        file_path = row[\"file_path\"]\n",
    "        emotion = row[\"emotion\"]\n",
    "        \n",
    "        output_file = os.path.join(output_dir, f\"{os.path.basename(file_path).split('.')[0]}.json\")\n",
    "        \n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Transcript exists for {file_path}\")\n",
    "            with open(output_file, 'r') as f:\n",
    "                transcript_data = json.load(f)\n",
    "            transcript_text = transcript_data[\"text\"]\n",
    "        else:\n",
    "            print(f\"Processing {file_path}...\")\n",
    "            \n",
    "            try:\n",
    "                segments, info = model.transcribe(file_path, beam_size=5, language=\"en\", condition_on_previous_text=False)\n",
    "                transcript_text = \" \".join([segment.text for segment in segments])\n",
    "                transcript_data = {\"text\": transcript_text}\n",
    "                with open(output_file, 'w') as f:\n",
    "                    json.dump(transcript_data, f)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {str(e)}\")\n",
    "                transcript_text = \"\"\n",
    "        \n",
    "        transcripts.append({\n",
    "            \"file_path\": file_path,\n",
    "            \"emotion\": emotion,\n",
    "            \"transcript\": transcript_text\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_df = add_transcripts_to_ravdess(ravdess_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def extract_audio_features(file_path, max_length=128000, sr=16000, n_mfcc=13, n_mels=128):\n",
    "    \"\"\"Extract audio features (MFCC and Mel spectrogram) from an audio file\"\"\"\n",
    "    audio, _ = librosa.load(file_path, sr=sr)\n",
    "    \n",
    "    if len(audio) > max_length:\n",
    "        audio = audio[:max_length]\n",
    "    else:\n",
    "        audio = np.pad(audio, (0, max_length - len(audio)), 'constant')\n",
    "    \n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    \n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels)\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "    \n",
    "    mfccs = (mfccs - np.mean(mfccs)) / (np.std(mfccs) + 1e-8)\n",
    "    log_mel_spec = (log_mel_spec - np.mean(log_mel_spec)) / (np.std(log_mel_spec) + 1e-8)\n",
    "    \n",
    "    features = {\n",
    "        'mfccs': mfccs,\n",
    "        'log_mel_spec': log_mel_spec\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_dataset_features(df):\n",
    "    \"\"\"Extract features for all audio files in the dataset\"\"\"\n",
    "    audio_features = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        file_path = row[\"file_path\"]\n",
    "        emotion = row[\"emotion\"]\n",
    "        transcript = row[\"transcript\"]\n",
    "        \n",
    "        features = extract_audio_features(file_path)\n",
    "        \n",
    "        audio_features.append({\n",
    "            \"file_path\": file_path,\n",
    "            \"emotion\": emotion,\n",
    "            \"transcript\": transcript,\n",
    "            \"mfccs\": features[\"mfccs\"],\n",
    "            \"log_mel_spec\": features[\"log_mel_spec\"]\n",
    "        })\n",
    "    \n",
    "    return audio_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "def extract_text_features(transcripts, max_length=64):\n",
    "    \"\"\"Extract text features using BERT\"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    model.eval()\n",
    "    \n",
    "    text_features = []\n",
    "    \n",
    "    for transcript in transcripts:\n",
    "        inputs = tokenizer(transcript, return_tensors=\"pt\", max_length=max_length, \n",
    "                          padding=\"max_length\", truncation=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        text_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "        text_features.append(text_embedding[0])\n",
    "    \n",
    "    return np.array(text_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "dataset_features = prepare_dataset_features(transcripts_df)\n",
    "\n",
    "# Save features\n",
    "import pickle\n",
    "with open('ravdess_features.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset_features, f)\n",
    "\n",
    "# Extract text features\n",
    "transcripts = transcripts_df[\"transcript\"].tolist()\n",
    "text_features = extract_text_features(transcripts)\n",
    "\n",
    "# Save text features\n",
    "np.save('ravdess_text_features.npy', text_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collating final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"Dataset for multimodal emotion recognition\"\"\"\n",
    "    def __init__(self, features, text_features, transform=None):\n",
    "        self.features = features\n",
    "        self.text_features = text_features\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.emotions = [item[\"emotion\"] for item in features]\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.encoded_emotions = self.label_encoder.fit_transform(self.emotions)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_feature = self.features[idx][\"log_mel_spec\"]\n",
    "        text_feature = self.text_features[idx]\n",
    "        emotion = self.encoded_emotions[idx]\n",
    "        \n",
    "        audio_tensor = torch.FloatTensor(audio_feature).unsqueeze(0)\n",
    "        text_tensor = torch.FloatTensor(text_feature)\n",
    "        emotion_tensor = torch.tensor(emotion, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            audio_tensor = self.transform(audio_tensor)\n",
    "        \n",
    "        return {\n",
    "            \"audio\": audio_tensor,\n",
    "            \"text\": text_tensor,\n",
    "            \"emotion\": emotion_tensor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dataset = EmotionDataset(dataset_features, text_features)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_indices, val_indices = train_test_split(\n",
    "    list(range(len(emotion_dataset))), \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=[item[\"emotion\"] for item in dataset_features]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "train_dataset = Subset(emotion_dataset, train_indices)\n",
    "val_dataset = Subset(emotion_dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    \"\"\"Audio encoder with fixed dimensions\"\"\"\n",
    "    def __init__(self, input_channels=1, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((4, 4))\n",
    "        )\n",
    "        self.fc = nn.Linear(64 * 4 * 4, hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Text encoder for emotion recognition\"\"\"\n",
    "    def __init__(self, input_dim=768, hidden_dim=64):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    \"\"\"Multimodal emotion classifier using audio and text features\"\"\"\n",
    "    def __init__(self, num_emotions=8, audio_hidden_dim=64, text_hidden_dim=64, fusion_dim=128):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "        self.audio_encoder = AudioEncoder(hidden_dim=audio_hidden_dim)\n",
    "        self.text_encoder = TextEncoder(hidden_dim=text_hidden_dim)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(audio_hidden_dim + text_hidden_dim, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(fusion_dim, num_emotions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, audio, text):\n",
    "        audio_features = self.audio_encoder(audio)\n",
    "        text_features = self.text_encoder(text)\n",
    "        combined_features = torch.cat((audio_features, text_features), dim=1)\n",
    "        output = self.fusion(combined_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, train_loader, val_loader, dataset, num_epochs=100, learning_rate=0.001):\n",
    "    \"\"\"Train the emotion classification model\"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_true = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            audio = batch[\"audio\"].to(device)\n",
    "            text = batch[\"text\"].to(device)\n",
    "            emotions = batch[\"emotion\"].to(device)\n",
    "            \n",
    "            outputs = model(audio, text)\n",
    "            loss = criterion(outputs, emotions)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_preds.extend(predicted.cpu().numpy())\n",
    "            train_true.extend(emotions.cpu().numpy())\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = accuracy_score(train_true, train_preds)\n",
    "        train_f1 = f1_score(train_true, train_preds, average='weighted')\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                audio = batch[\"audio\"].to(device)\n",
    "                text = batch[\"text\"].to(device)\n",
    "                emotions = batch[\"emotion\"].to(device)\n",
    "                \n",
    "                outputs = model(audio, text)\n",
    "                loss = criterion(outputs, emotions)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "                val_true.extend(emotions.cpu().numpy())\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = accuracy_score(val_true, val_preds)\n",
    "        val_f1 = f1_score(val_true, val_preds, average='weighted')\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_emotion_model.pt\")\n",
    "            print(\"Saved best model!\")\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_history.png\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            audio = batch[\"audio\"].to(device)\n",
    "            text = batch[\"text\"].to(device)\n",
    "            emotions = batch[\"emotion\"].to(device)\n",
    "            \n",
    "            outputs = model(audio, text)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_true.extend(emotions.cpu().numpy())\n",
    "    \n",
    "    cm = confusion_matrix(all_true, all_preds)\n",
    "    emotion_labels = dataset.label_encoder.classes_\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=emotion_labels, yticklabels=emotion_labels)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, best_val_acc, train_losses, val_losses,\n",
    "                    train_accuracies, val_accuracies, no_improvement_counter, filename):\n",
    "    \"\"\"Save training checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'no_improvement_counter': no_improvement_counter\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(\"checkpoints\", filename))\n",
    "    print(f\"\\nCheckpoint saved as {filename}\")\n",
    "\n",
    "\n",
    "def train_model_es(model, train_loader, val_loader, dataset, num_epochs=100, learning_rate=0.001, \n",
    "                   patience=5, delta=0.001, max_checkpoints=3):\n",
    "    \"\"\"Train the emotion classification model with early stopping and checkpoint management\"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    no_improvement_counter = 0\n",
    "    saved_checkpoints = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_true = []\n",
    "\n",
    "        for batch in train_loader:\n",
    "            audio = batch[\"audio\"].to(device)\n",
    "            text = batch[\"text\"].to(device)\n",
    "            emotions = batch[\"emotion\"].to(device)\n",
    "            \n",
    "            outputs = model(audio, text)\n",
    "            loss = criterion(outputs, emotions)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_preds.extend(predicted.cpu().numpy())\n",
    "            train_true.extend(emotions.cpu().numpy())\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = accuracy_score(train_true, train_preds)\n",
    "        train_f1 = f1_score(train_true, train_preds, average='weighted')\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                audio = batch[\"audio\"].to(device)\n",
    "                text = batch[\"text\"].to(device)\n",
    "                emotions = batch[\"emotion\"].to(device)\n",
    "                \n",
    "                outputs = model(audio, text)\n",
    "                loss = criterion(outputs, emotions)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "                val_true.extend(emotions.cpu().numpy())\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = accuracy_score(val_true, val_preds)\n",
    "        val_f1 = f1_score(val_true, val_preds, average='weighted')\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        if val_acc > (best_val_acc + delta):\n",
    "            print(f\"Validation accuracy improved from {best_val_acc:.4f} to {val_acc:.4f}\")\n",
    "            best_val_acc = val_acc\n",
    "            no_improvement_counter = 0\n",
    "            \n",
    "            checkpoint_name = f\"model_{val_acc:.4f}_epoch{epoch+1}_checkpoint.pt\"\n",
    "            save_checkpoint(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                epoch=epoch,\n",
    "                best_val_acc=best_val_acc,\n",
    "                train_losses=train_losses,\n",
    "                val_losses=val_losses,\n",
    "                train_accuracies=train_accuracies,\n",
    "                val_accuracies=val_accuracies,\n",
    "                no_improvement_counter=no_improvement_counter,\n",
    "                filename=checkpoint_name\n",
    "            )\n",
    "            \n",
    "            checkpoint_name = os.path.join(\"checkpoints\", checkpoint_name)\n",
    "            saved_checkpoints.append((val_acc, epoch+1, checkpoint_name))\n",
    "            saved_checkpoints.sort(key=lambda x: (-x[0], -x[1]))\n",
    "            \n",
    "            while len(saved_checkpoints) > max_checkpoints:\n",
    "                removed_acc, removed_epoch, removed_file = saved_checkpoints.pop()\n",
    "                try:\n",
    "                    os.remove(removed_file)\n",
    "                    print(f\"Removed old checkpoint: {removed_file}\")\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"Warning: Could not find file {removed_file} to delete\")\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "            print(f\"No improvement for {no_improvement_counter}/{patience} epochs\")\n",
    "            if no_improvement_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}!\")\n",
    "                break\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_history.png\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            audio = batch[\"audio\"].to(device)\n",
    "            text = batch[\"text\"].to(device)\n",
    "            emotions = batch[\"emotion\"].to(device)\n",
    "            \n",
    "            outputs = model(audio, text)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy\n",
    "                             ())\n",
    "            all_true.extend(emotions.cpu().numpy())\n",
    "    \n",
    "    cm = confusion_matrix(all_true, all_preds)\n",
    "    emotion_labels = dataset.label_encoder.classes_\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=emotion_labels, yticklabels=emotion_labels)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_emotions = len(emotion_dataset.label_encoder.classes_)\n",
    "model = EmotionClassifier(num_emotions=num_emotions)\n",
    "\n",
    "trained_model = train_model_es(model, train_loader, val_loader, emotion_dataset, num_epochs=400, learning_rate=0.001, patience=40, delta=0.01, max_checkpoints=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filename):\n",
    "    path = os.path.join(\"checkpoints\", filename)\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return {\n",
    "        'epoch': checkpoint['epoch'],\n",
    "        'best_val_acc': checkpoint['best_val_acc'],\n",
    "        'train_losses': checkpoint['train_losses'],\n",
    "        'val_losses': checkpoint['val_losses'],\n",
    "        'train_accuracies': checkpoint['train_accuracies'],\n",
    "        'val_accuracies': checkpoint['val_accuracies'],\n",
    "        'no_improvement_counter': checkpoint['no_improvement_counter']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def load_best_model(model_class=EmotionClassifier, checkpoint_dir='checkpoints', device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    checkpoints = glob.glob(os.path.join(checkpoint_dir, 'model_*.pt'))\n",
    "    if not checkpoints:\n",
    "        raise ValueError(\"No checkpoints found\")\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_checkpoint = None\n",
    "    \n",
    "    for ckpt in checkpoints:\n",
    "        data = torch.load(ckpt, map_location=device)\n",
    "        min_loss = min(data['val_losses'])\n",
    "        if min_loss < best_loss:\n",
    "            best_loss = min_loss\n",
    "            best_checkpoint = data\n",
    "            name = ckpt\n",
    "    \n",
    "    model = model_class().to(device)\n",
    "    model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded model: {name}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_best_model(EmotionClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_emotion(audio_path):\n",
    "    whisper_model = WhisperModel(\"distil-large-v3\", compute_type=\"float16\" if torch.cuda.is_available() else \"float32\")\n",
    "    segments, info = whisper_model.transcribe(audio_path, beam_size=5, language=\"en\", condition_on_previous_text=False)\n",
    "    transcript = \" \".join([segment.text for segment in segments])\n",
    "    \n",
    "    audio_features = extract_audio_features(audio_path)\n",
    "    audio_tensor = torch.FloatTensor(audio_features[\"log_mel_spec\"]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    text_features = extract_text_features([transcript])[0]\n",
    "    text_tensor = torch.FloatTensor(text_features).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(audio_tensor, text_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        conf, pred = torch.max(probs, 1)\n",
    "    \n",
    "    probs_list = [(label, prob.item()) \n",
    "                for label, prob in zip(emotion_dataset.label_encoder.classes_, probs.squeeze())]\n",
    "    sorted_probs = sorted(probs_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"\\n{' EMOTION PREDICTION RESULTS ':=^40}\")\n",
    "    print(f\"Audio File: {os.path.basename(audio_path)}\")\n",
    "    print(f\"\\nTranscript: {transcript}\\n\")\n",
    "    print(f\"{'Top Predictions':-^30}\")\n",
    "    for i, (emotion, prob) in enumerate(sorted_probs[:3], 1):\n",
    "        print(f\"{i}. {emotion.upper():<12} {prob:.2%}\")\n",
    "    print(f\"{'':-^30}\")\n",
    "    print(f\"Predicted Emotion: {sorted_probs[0][0].upper()} (confidence: {conf.item():.2%})\")\n",
    "    print(f\"{'':=^40}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"emotion\": sorted_probs[0][0],\n",
    "        \"confidence\": conf.item(),\n",
    "        \"transcript\": transcript,\n",
    "        \"probabilities\": sorted_probs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_emotion('Audio_Speech_Actors_01-24/Actor_05/03-01-01-01-01-02-05.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying with a simple UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def gradio_detect_emotion(audio_path):\n",
    "    result = detect_emotion(audio_path)\n",
    "    \n",
    "    top_emotion = f\"{result['emotion'].upper()} ({result['confidence']:.2%})\"\n",
    "    prob_dict = {k.upper(): v for k, v in result['probabilities']}\n",
    "    \n",
    "    output_text = f\"\"\"\n",
    "{'-'*40}\n",
    "Audio File: {os.path.basename(audio_path)}\n",
    "{'-'*40}\n",
    "Transcript: {result['transcript']}\n",
    "{'-'*40}\n",
    "Top Predictions:\n",
    "\"\"\" + \"\\n\".join([f\"{i}. {emotion.upper():<12} {prob:.2%}\" \n",
    "               for i, (emotion, prob) in enumerate(result['probabilities'][:3], 1)]) + f\"\"\"\n",
    "{'-'*40}\n",
    "Final Prediction: {top_emotion}\n",
    "{'-'*40}\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"transcript\": result['transcript'],\n",
    "        \"emotion\": top_emotion,\n",
    "        \"probabilities\": prob_dict\n",
    "    }, output_text\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"Speech Emotion Recognition\") as demo:\n",
    "    gr.Markdown(\"# ðŸŽ¤ Real-Time Speech Emotion Analysis\")\n",
    "    gr.Markdown(\"Upload an audio file or record using your microphone\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(\n",
    "            sources=[\"microphone\", \"upload\"],\n",
    "            type=\"filepath\",\n",
    "            label=\"Input Audio\",\n",
    "            waveform_options=gr.WaveformOptions(waveform_color=\"#FF6B6B\")\n",
    "        )\n",
    "        \n",
    "    with gr.Row():\n",
    "        text_output = gr.Textbox(label=\"Transcript\", interactive=False)\n",
    "        emotion_output = gr.Textbox(label=\"Predicted Emotion\", interactive=False)\n",
    "        \n",
    "    plot_output = gr.BarPlot(\n",
    "        label=\"Emotion Probabilities\",\n",
    "        x=\"Emotion\",\n",
    "        y=\"Probability\",\n",
    "        color=\"Emotion\",\n",
    "        height=300\n",
    "    )\n",
    "    \n",
    "    console_output = gr.Textbox(label=\"Analysis Details\", interactive=False)\n",
    "    \n",
    "    audio_input.change(\n",
    "        fn=gradio_detect_emotion,\n",
    "        inputs=audio_input,\n",
    "        outputs=[{\"transcript\": text_output, \"emotion\": emotion_output, \"probabilities\": plot_output}, console_output]\n",
    "    )\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_detect_emotion(audio_path):\n",
    "    result = detect_emotion(audio_path)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"Emotion\": [e[0].upper() for e in result['probabilities']],\n",
    "        \"Probability\": [e[1] for e in result['probabilities']]\n",
    "    })\n",
    "    \n",
    "    top_emotion = f\"{result['emotion'].upper()} ({result['confidence']:.2%})\"\n",
    "    output_text = f\"\"\"\n",
    "Transcript: {result['transcript']}\n",
    "Predicted Emotion: {top_emotion}\n",
    "Top 3 Probabilities:\n",
    "\"\"\" + \"\\n\".join([f\"{i}. {e[0].upper():<12} {e[1]:.2%}\" \n",
    "               for i, e in enumerate(result['probabilities'][:3], 1)])\n",
    "\n",
    "    return result['transcript'], top_emotion, df, output_text\n",
    "\n",
    "with gr.Blocks(title=\"Speech Emotion Recognition\") as demo:\n",
    "    gr.Markdown(\"# ðŸŽ¤ Real-Time Speech Emotion Analysis\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(\n",
    "            sources=[\"microphone\", \"upload\"],\n",
    "            type=\"filepath\",\n",
    "            label=\"Input Audio\"\n",
    "        )\n",
    "        \n",
    "    with gr.Row():\n",
    "        text_output = gr.Textbox(label=\"Transcript\")\n",
    "        emotion_output = gr.Textbox(label=\"Predicted Emotion\")\n",
    "        \n",
    "    plot_output = gr.BarPlot(\n",
    "        label=\"Emotion Probabilities\",\n",
    "        x=\"Emotion\",\n",
    "        y=\"Probability\",\n",
    "        title=\"Emotion Distribution\",\n",
    "        height=300\n",
    "    )\n",
    "    \n",
    "    console_output = gr.Textbox(label=\"Analysis Details\")\n",
    "    \n",
    "    audio_input.change(\n",
    "        fn=gradio_detect_emotion,\n",
    "        inputs=audio_input,\n",
    "        outputs=[text_output, emotion_output, plot_output, console_output]\n",
    "    )\n",
    "\n",
    "demo.launch()\n",
    "# demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
